{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb247737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install / setup cell (RUN FIRST)\n",
    "import sys\n",
    "!{sys.executable} -m pip install -U \\\n",
    "  langchain \\\n",
    "  langchain-core \\\n",
    "  langchain-community \\\n",
    "  langchain-text-splitters \\\n",
    "  langchain-huggingface \\\n",
    "  langchain-google-vertexai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6332d09c",
   "metadata": {},
   "source": [
    "‚¨ÜÔ∏è **After this cell completes:**\n",
    "\n",
    "**Restart the kernel NOW**, then continue below.\n",
    "\n",
    "Yes, restarting twice is intentional when upgrading LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68895846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports (LangChain 1.x compatible)\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Suppress tqdm warning about ipywidgets\n",
    "warnings.filterwarnings('ignore', message='IProgress not found')\n",
    "\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "from langchain_google_vertexai import (\n",
    "    ChatVertexAI,\n",
    "    VertexAIEmbeddings,\n",
    "    VertexAI,\n",
    ")\n",
    "\n",
    "print('Environment ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45024fb2",
   "metadata": {},
   "source": [
    "## Step 1: Define metadata wrapper function\n",
    "\n",
    "This function formats and adds metadata to chunks for the Oracle Vector Store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d06c796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to format and add metadata to Oracle 23ai Vector Store\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def chunks_to_docs_wrapper(row: dict) -> Document:\n",
    "    \"\"\"\n",
    "    Converts text into a Document object suitable for ingestion into Oracle Vector Store.\n",
    "    \"\"\"\n",
    "    metadata = {'id': row['id'], 'link': row['link']}\n",
    "    return Document(page_content=row['text'], metadata=metadata)\n",
    "\n",
    "print(\"Successfully defined metadata wrapper\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3233600d",
   "metadata": {},
   "source": [
    "## Step 2: Connect to Oracle Database 23ai\n",
    "\n",
    "Update with your credentials:\n",
    "- **Username**\n",
    "- **Password** \n",
    "- **Connection String** (from tnsnames.ora)\n",
    "- **Wallet Password**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec5ed69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import oracledb\n",
    "\n",
    "un = \"ADMIN\"           # Enter Username\n",
    "pw = \"password\"           # Enter Password\n",
    "dsn = 'connection string' # Enter Connection String\n",
    "wpwd = \"wallet password\"  # Enter Wallet Password\n",
    "dsn = 'paulparkdb_high' # Enter Connection String (from tnsnames.ora)\n",
    "wpwd = \"YourWalletPassword123#\"  # Enter Wallet Password\n",
    "\n",
    "# Use absolute path to wallet directory\n",
    "wallet_path = os.path.expanduser('~/wallet')\n",
    "\n",
    "connection = oracledb.connect(\n",
    "    config_dir=wallet_path,\n",
    "    user=un, \n",
    "    password=pw, \n",
    "    dsn=dsn,\n",
    "    wallet_location=wallet_path,\n",
    "    wallet_password=wpwd\n",
    ")\n",
    "\n",
    "print(\"Successfully connected to Oracle Database 23ai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4057596",
   "metadata": {},
   "source": [
    "## Step 3: Load PDF Document\n",
    "\n",
    "Load the PDF document and display basic information about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4903d8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "\n",
    "pdf = PdfReader('oracle-ai-database-26ai-new-features-guide.pdf')\n",
    "print(f\"The number of pages in this document is {len(pdf.pages)}\")\n",
    "print(\"\\n--- First Page Preview ---\")\n",
    "print(pdf.pages[0].extract_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0b501d",
   "metadata": {},
   "source": [
    "## Step 4: Transform PDF to Text\n",
    "\n",
    "Extract text from all pages of the PDF document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a171a57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pdf is not None:\n",
    "    print(\"Transforming the PDF document to text...\")\n",
    "    text = \"\"\n",
    "    for page in pdf.pages:\n",
    "        text += page.extract_text()\n",
    "    print(f\"Successfully transformed {len(pdf.pages)} pages to text\")\n",
    "    print(f\"Total text length: {len(text)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf747a85",
   "metadata": {},
   "source": [
    "## Step 5: Split Text into Chunks\n",
    "\n",
    "Chunk size: 800 characters with 100 character overlap.\n",
    "\n",
    "**Note:** Chunk sizes vary depending on document type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed8fb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_text(text)\n",
    "print(f\"Created {len(chunks)} chunks\")\n",
    "print(\"\\n--- First Chunk Preview ---\")\n",
    "print(chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa421cb9",
   "metadata": {},
   "source": [
    "## Step 6: Create Documents with Metadata\n",
    "\n",
    "Wrap each chunk with metadata (id and link) for storage in the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740749ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    chunks_to_docs_wrapper({\n",
    "        'id': f'{page_num}', \n",
    "        'link': f'Page {page_num}', \n",
    "        'text': text\n",
    "    }) \n",
    "    for page_num, text in enumerate(chunks)\n",
    "]\n",
    "\n",
    "print(f\"Created {len(docs)} documents with metadata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d775079",
   "metadata": {},
   "source": [
    "## Step 7: Initialize Vertex AI\n",
    "\n",
    "Configure your Google Cloud project and region for Vertex AI services."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a2e46d",
   "metadata": {},
   "source": [
    "## Step 6a: Authenticate with GCP (First Time Setup)\n",
    "\n",
    "**Run this ONCE on your GCP VM to set up authentication:**\n",
    "\n",
    "```bash\n",
    "# In the VM terminal (not in notebook):\n",
    "gcloud auth application-default login --no-launch-browser\n",
    "```\n",
    "\n",
    "Follow the URL, copy the authorization code, and paste it back.\n",
    "\n",
    "**OR** if you have a service account key file:\n",
    "```bash\n",
    "export GOOGLE_APPLICATION_CREDENTIALS=\"/path/to/your/service-account-key.json\"\n",
    "```\n",
    "\n",
    "‚ö†Ô∏è **After authentication, restart the kernel and re-run from the top.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f31338c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "\n",
    "PROJECT_ID = \"adb-pm-prod\"  # Update with your GCP Project ID\n",
    "REGION = \"us-central1\"             # Update with your region (us-central1 has most models)\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=REGION)\n",
    "print(f\"Initialized Vertex AI for project: {PROJECT_ID} in region: {REGION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4a91aa",
   "metadata": {},
   "source": [
    "## Step 8: Embed and Store Vectors in Oracle 23ai\n",
    "\n",
    "Using **VertexAIEmbeddings** model and **DOT_PRODUCT** distance strategy for similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1087fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores.oraclevs import OracleVS\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "\n",
    "embeddings = VertexAIEmbeddings(model_name=\"text-embedding-004\")\n",
    "\n",
    "s1time = time.time()\n",
    "print(f\"Vectorizing and inserting {len(docs)} chunks into Oracle Database 23ai...\")\n",
    "print(\"Processing in smaller batches to avoid token limits...\")\n",
    "\n",
    "# Process in batches of 50 (to stay under 20k token limit)\n",
    "batch_size = 50\n",
    "knowledge_base = None\n",
    "\n",
    "for i in range(0, len(docs), batch_size):\n",
    "    batch = docs[i:i + batch_size]\n",
    "    batch_num = (i // batch_size) + 1\n",
    "    total_batches = (len(docs) + batch_size - 1) // batch_size\n",
    "    print(f\"  Processing batch {batch_num}/{total_batches} ({len(batch)} chunks)...\")\n",
    "    \n",
    "    if knowledge_base is None:\n",
    "        # Create the vector store with first batch\n",
    "        knowledge_base = OracleVS.from_documents(\n",
    "            batch,\n",
    "            embeddings,\n",
    "            client=connection,\n",
    "            table_name=\"RAG_TAB\",\n",
    "            distance_strategy=DistanceStrategy.DOT_PRODUCT\n",
    "        )\n",
    "    else:\n",
    "        # Add remaining batches to existing store\n",
    "        knowledge_base.add_documents(batch)\n",
    "\n",
    "s2time = time.time()\n",
    "print(f\"‚úì Vectorizing and inserting chunks duration: {round(s2time - s1time, 1)} sec.\")\n",
    "print(f\"‚úì Successfully stored {len(docs)} chunks in Oracle Database 23ai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665addad",
   "metadata": {},
   "source": [
    "## Step 9: Verify Data in Oracle Database\n",
    "\n",
    "Query the RAG_TAB table to confirm vectors were inserted successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353be3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = \"RAG_TAB\"\n",
    "\n",
    "with connection.cursor() as cursor:\n",
    "    query = f\"SELECT * FROM {table_name}\"\n",
    "    cursor.execute(query)\n",
    "    rows = cursor.fetchall()\n",
    "    \n",
    "    print(f\"Total rows in {table_name}: {len(rows)}\")\n",
    "    print(\"\\n--- Sample Rows (first 3) ---\")\n",
    "    for row in rows[:3]:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb76fc5a",
   "metadata": {},
   "source": [
    "## Step 10: Define User Question\n",
    "\n",
    "Ask a question about the document content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6ed180",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_question = 'Tell me more about JSON Relational Duality'\n",
    "print(f\"The prompt to the LLM will be: {user_question}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0fc0fa",
   "metadata": {},
   "source": [
    "## Step 11: Perform Similarity Search\n",
    "\n",
    "Test the vector similarity search to find relevant chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e2a4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if user_question:\n",
    "    s3time = time.time()\n",
    "    result_chunks = knowledge_base.similarity_search(user_question, k=5)\n",
    "    s4time = time.time()\n",
    "    \n",
    "    print(f\"‚úì Search duration: {round(s4time - s3time, 1)} sec.\")\n",
    "    print(f\"\\nFound {len(result_chunks)} relevant chunks:\")\n",
    "    for i, chunk in enumerate(result_chunks, 1):\n",
    "        print(f\"\\nChunk {i}: {chunk.page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b346a9d",
   "metadata": {},
   "source": [
    "## Step 12: Configure Gemini LLM\n",
    "\n",
    "Set up Vertex AI's **Gemini 2.0 Flash** model for generating responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b69f6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatVertexAI(\n",
    "    model_name=\"gemini-2.5-flash\",\n",
    "    max_output_tokens=8192,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    top_k=40,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"‚úì Configured Gemini 2.5 Flash model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9332d6",
   "metadata": {},
   "source": [
    "## Step 13: Build RAG Prompt Template\n",
    "\n",
    "Create the prompt template and retriever for the RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cd31c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "            {context} Question: {question} \"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "retriever = knowledge_base.as_retriever(search_kwargs={\"k\": 10})\n",
    "print(\"The template is:\", template)\n",
    "print(retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a87a2b",
   "metadata": {},
   "source": [
    "## Step 14: Execute RAG Chain\n",
    "\n",
    "Invoke the complete RAG pipeline to generate the final response.\n",
    "\n",
    "The chain:\n",
    "1. Retrieves relevant context from Oracle Vector DB\n",
    "2. Constructs prompt with question + context\n",
    "3. Sends to Gemini LLM for response generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1e35d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "s5time = time.time()\n",
    "print(\"Sending prompt and RAG context to Gemini LLM...\")\n",
    "print(f\"Question: {user_question}\\n\")\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "response = chain.invoke(user_question)\n",
    "\n",
    "s6time = time.time()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"RESPONSE:\")\n",
    "print(\"=\" * 80)\n",
    "print(response)\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n‚úì LLM response duration: {round(s6time - s5time, 1)} sec.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f58e1c",
   "metadata": {},
   "source": [
    "## üéâ Congratulations!\n",
    "\n",
    "You've successfully built a complete RAG application with:\n",
    "- **Oracle Database 23ai** for vector storage\n",
    "- **Vertex AI Embeddings** for vectorization  \n",
    "- **Gemini 2.0 Flash** for response generation\n",
    "- **LangChain 1.x** for orchestration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc11e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"üéâ RAG Application Complete!\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n‚úì PDF loaded and processed\")\n",
    "print(\"‚úì Text chunked and vectorized\") \n",
    "print(\"‚úì Vectors stored in Oracle Database 23ai\")\n",
    "print(\"‚úì Similarity search working\")\n",
    "print(\"‚úì Gemini LLM integration successful\")\n",
    "print(\"\\nYou've completed the RAG application lab!\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
